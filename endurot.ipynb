{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autonomous driving using DQN  baseline methods *(endurot env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries used                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import gym\n",
    "%matplotlib inline\n",
    "from Agent import agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action space Discrete(9)\n",
      "observation space Box(210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load the environement. Here you need to adjust the filename.\n",
    "env = gym.make('Enduro-v0')\n",
    "env.reset()\n",
    "action_size=9\n",
    "\n",
    "print(\"action space\",env.action_space)\n",
    "print(\"observation space\",env.observation_space)\n",
    "# 0 = straight\n",
    "# 1 = left\n",
    "# 2 = right\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_poc(screen):\n",
    "    screen = np.asarray(screen[70:125,60:])\n",
    "    for i in range(len(screen)):\n",
    "        for j in range(len(screen[0])):\n",
    "            screen[i,j,0] = 0.2989 * screen[i,j,0] + 0.5870 * screen[i,j,1] + 0.1140 * screen[i,j,2]\n",
    "    screen=screen[:,:,0].reshape(1,55,100)\n",
    " \n",
    "\n",
    "    return screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_episode in range(0):\n",
    "    current_screen = post_poc(env.reset()) \n",
    "    last_screen = current_screen \n",
    "    for t in range(100):\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        #print(\"action:\",info)\n",
    "        current_screen, reward, done, info = env.step(action)\n",
    "        current_screen=post_poc(current_screen)\n",
    "        next_state=last_screen-current_screen\n",
    "        last_screen=current_screen     \n",
    "        plt.imshow(next_state[0])\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#    0 - walk forward\n",
    "#    1 - walk backward\n",
    "#    2 - turn left\n",
    "#    3 - turn right "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Test(env,agent,file_name,max_t=1000):\n",
    "    \"\"\" Visualize agent using saved checkpoint. \"\"\"\n",
    "    # load saved weights\n",
    "    agent.qnetwork_local.load_state_dict(torch.load(file_name))\n",
    " \n",
    "    scores = []                        # list containing scores from each episode\n",
    "    score = 0\n",
    "\n",
    "    env_info = env.reset()\n",
    "    score = 0\n",
    "    t=0\n",
    "    for i_episode in range(2):\n",
    "        current_screen = post_poc(env.reset()) \n",
    "        last_screen = current_screen\n",
    "        state=current_screen-last_screen\n",
    "        for t in range(max_t):\n",
    "            env.render()\n",
    "            action = agent.act(state)\n",
    "            #print(\"action:\",info)\n",
    "            current_screen, reward, done, info = env.step(action)\n",
    "            current_screen=post_poc(current_screen)\n",
    "            next_state=last_screen-current_screen\n",
    "            state=next_state\n",
    "            last_screen=current_screen \n",
    "            # roll over the state to next time step\n",
    "            score += reward      # update the score\n",
    "            scores.append(score)              # save most recent score\n",
    "            if done or t>= max_t:\n",
    "                break\n",
    "        print(\"Average reward par step:\",score/t)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(x,y,xlabel='Episode #',ylabel='Score',title=\"Results\"):\n",
    "  # plot the scores\n",
    "  fig = plt.figure()\n",
    "  ax = fig.add_subplot(111)\n",
    "  plt.plot(x, y)\n",
    "  plt.ylabel(ylabel)\n",
    "  plt.xlabel(xlabel)\n",
    "  plt.title(title, fontsize=16, fontweight='bold')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size=(55,100)\n",
    "def train(env,agent,dqn,file_save='checkpoint.pth',n_episodes=700, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "  \n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    " \n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_mean=[]                     # list containing the mean score at each step\n",
    "    scores_std=[]                      # list containing standart deviation at each step\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        current_screen = post_poc(env.reset())\n",
    "        last_screen = current_screen\n",
    "        state=current_screen-last_screen\n",
    "        \n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            plt.imshow(state[0])\n",
    "            env.render()\n",
    "            action = agent.act(state, eps)\n",
    "            current_screen, reward, done, info = env.step(action)\n",
    "            current_screen = post_poc(current_screen)\n",
    "            next_state = current_screen - last_screen\n",
    "             # information about the environment\n",
    "            #print(\"vector len\",len(state[0][0]),len(state[0][0][0]),len(next_state[0][0]),len(next_state[0][0][0]))\n",
    "            agent.step(state, action, reward, next_state, done,dqn)\n",
    "            last_screen = current_screen\n",
    "            score += reward      # update the score\n",
    "            state = next_state   # roll over the state to next time step\n",
    "            if done:             # exit loop if episode finished\n",
    "                break\n",
    " \n",
    "        scores_window.append(score)                     # save most recent score\n",
    "        scores.append(score)                            # save most recent score\n",
    "        scores_mean.append(np.mean(scores_window))      # save most recent mean score last 100 scores\n",
    "        scores_std.append(np.std(scores_window))        # save most recent standart deviation over last 100 scores        \n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "\n",
    "\n",
    "        if i_episode % 10 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "             \n",
    "        #if i_episode % 10 == 0:         \n",
    "           #print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "\n",
    "        if np.mean(scores_window)>=13.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), file_save)\n",
    "            break\n",
    "\n",
    "    return scores,scores_mean, scores_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QNetwork(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(5, 5), stride=(2, 2))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2))\n",
      "  (head): Linear(in_features=2112, out_features=9, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "dqn_agent = agent(state_size, action_size,duel=False, fc1_units=64,fc2_units=64,seed=0)\n",
    "dqn_scores,dqn_scores_mean, dqn_scores_std  = train(env,dqn_agent,dqn=True,file_save='dqn_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(np.arange(len(dqn_scores)),dqn_scores,xlabel='Episode #',ylabel='Score',title=\"DQN\")\n",
    "plot_data(np.arange(len(dqn_scores_mean)),dqn_scores_mean,xlabel='Episode #',ylabel='Score avg',title=\"DQN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddqn_agent = agent(state_size, action_size, duel=False, fc1_units=64,fc2_units=64,seed=0)\n",
    "ddqn_scores,ddqn_scores_mean, ddqn_scores_std = train(env,brain_name,ddqn_agent,dqn=False,file_save='ddqn_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(np.arange(len(ddqn_scores)),ddqn_scores,xlabel='Episode #',ylabel='Score',title=\"DDQN\")\n",
    "plot_data(np.arange(len(ddqn_scores_mean)),ddqn_scores_mean,xlabel='Episode #',ylabel='Score avg',title=\"DDQN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duelling DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "duel_dqn_agent = agent(state_size, action_size,duel=True, fc1_units=64,fc2_units=64,seed=0)\n",
    "duel_dqn_scores,duel_dqn_scores_mean,duel_dqn_scores_std = train(env,brain_name,duel_dqn_agent,dqn=True,file_save='duel_dqn_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(np.arange(len(duel_dqn_scores)),duel_dqn_scores,ylabel='Score',title=\"Duel DQN\")\n",
    "plot_data(np.arange(len(duel_dqn_scores_mean)),duel_dqn_scores_mean,ylabel='Score avg',title=\"Duel DQN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duelling DDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duel_ddqn_agent = agent(state_size, action_size, duel=True, fc1_units=64,fc2_units=64,seed=0)\n",
    "duel_ddqn_scores,duel_ddqn_scores_mean,duel_ddqn_scores_std = train(env,brain_name,duel_ddqn_agent,dqn=False,file_save='duel_ddqn_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(np.arange(len(duel_ddqn_scores)),duel_ddqn_scores,ylabel='Score',title=\"Duel DDQN\")\n",
    "plot_data(np.arange(len(duel_ddqn_scores_mean)),duel_ddqn_scores_mean,ylabel='Score avg',title=\"Duel DDQN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(dqn_scores_mean)), dqn_scores_mean, color=\"red\", label=\"DQN\")\n",
    "plt.plot(np.arange(len(ddqn_scores_mean)), ddqn_scores_mean, color=\"green\", label=\"DDQN\")\n",
    "plt.plot(np.arange(len(duel_dqn_scores_mean)),duel_dqn_scores_mean,'-.',color=\"orange\",label=\"Duel_DQN\")\n",
    "plt.plot(np.arange(len(duel_ddqn_scores_mean)),duel_ddqn_scores_mean,'-.',color=\"blue\",label=\"Duel_DDQN\")\n",
    "plt.legend(loc='lower right')\n",
    "plt.title(\"Comparison\", fontsize=16, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "brain_name, brain, env_info, action_size, state_size = initialize_env(env,train_mode=False,brain_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = agent(state_size, action_size,duel=False, fc1_units=64,fc2_units=64,seed=0)\n",
    "file_name ='ddqn_checkpoint.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores=Test(env,brain_name,agent,file_name ,max_t=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(np.arange(len(scores)),scores,xlabel='Step #',ylabel='DQN Score')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
